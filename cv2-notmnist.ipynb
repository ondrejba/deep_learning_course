{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MI-MVI tutorial 2 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first tutorial, we introduced you to **Tensorflow**, a Deep Learning framework. You learned how to **define a computation graph**, and **create and initialize a Session**. Finally, you trained a simple classification model on a dataset of digits called **MNIST**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will experiments with various methods for the training of neural networks. Furthermore, we will show you how to use some advanced Tensorflow features like saving and loading models as well as visualizing the training. Lastly, we will learn how to regularize a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thank the authors of this [Udacity tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity) which was the main inspiration for this tutorial. We have reused some of their code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: the notMNIST dataset ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![notMNIST example](images/notMNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [notMNIST dataset](http://yaroslavvb.blogspot.cz/2011/09/notmnist-dataset.html) created by [Yaroslav Bulatov](https://www.blogger.com/profile/06139256691290554110) contains pictures of **letters from A - J** gathered from a multitude of publicly available fonts. The letters are in the same fromat as MNIST - 28x28 grayscale pictures. The dataset is harder than MNIST but small enough to be used on a laptop making it a perfect dataset for small-scale experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work only with letters A - D in order for our experiments to run faster. You can use *cv2-notmnist-prepare-data* to load all letters (you will need to make some changes) if you want to experiment with the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the preprocessed dataset from [this](https://drive.google.com/file/d/1FE7thwYRktH-D8wi_4dYXsGPNG_Qno10/view?usp=sharing) link and place it in the **data/notMNIST** directory. Alternatively, you can download and preprocess the dataset using the *cv2-notmnist-prepare-data* notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** Python modules we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have the preprocessed subset of notMNIST saved as a [pickle](). You can use the code snippet below to **load** it any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a subset of the notMNIST dataset\n",
    "data_root = 'data/notMNIST'\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  labels = save['labels']\n",
    "\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display** a couple of images so that we know what we are working with. If possible, always visualize your data preprocessing pipeline in order to spot bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"run again for different images\")\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "fig = plt.figure(1, figsize=(10, 10))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(1, 4), axes_pad=0.2)\n",
    "\n",
    "for i in range(4):\n",
    "    index = np.random.randint(0, train_dataset.shape[0])\n",
    "    \n",
    "    grid[i].imshow(train_dataset[index] / 255, interpolation=\"bilinear\", cmap=\"gray\")\n",
    "    grid[i].tick_params(axis='both', which='both', bottom='off', top='off', \n",
    "                    labelbottom='off', right='off', left='off', labelleft='off')\n",
    "\n",
    "    for key, value in labels.items():\n",
    "        if value == train_labels[index]:\n",
    "            print(key)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual pixels are encoded as grayscale values with integers in the range [0-255]. To help the classfier to converge, we **normalize** images so that they have zero mean and unit variance. \n",
    "\n",
    "**(Optional) Task**: Try skipping the normalization step or only subtracting means and see how it impacts the final performance (if the model trains at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train_dataset, axis=0)\n",
    "std = np.std(train_dataset, axis=0)\n",
    "\n",
    "train_dataset = (train_dataset - mean) / std\n",
    "valid_dataset = (valid_dataset - mean) / std\n",
    "test_dataset = (test_dataset - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we compute means and standard deviations over the training set. As expected, the training set has zero mean and unit variance after normalization. However, the means and variances of the validation and testing set slightly deviate. \n",
    "\n",
    "**Question**: Why do we use means and standard deviations computed over the training set to normalize the validation and testing sets?\n",
    "\n",
    "*Hint: generalization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"statistics after normalization\")\n",
    "print(\"training mean:\", np.round(np.mean(train_dataset), 5), \", variance:\", \n",
    "                        np.round(np.var(train_dataset), 5))\n",
    "print(\"validation mean:\", np.round(np.mean(valid_dataset), 5), \", variance:\", \n",
    "                          np.round(np.var(valid_dataset), 5))\n",
    "print(\"testing mean:\", np.round(np.mean(test_dataset), 5), \", variance:\", \n",
    "                       np.round(np.var(test_dataset), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffle** the pictures to make sure that each mini-batch is an **unbiased sample** of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "def unison_shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels = unison_shuffle(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = unison_shuffle(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = unison_shuffle(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a classification model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build a **neural network** for letter classification and train it with **batch gradient descent**, **stochastic gradient descent** and **mini-batch gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset stores each image as a Tensor of rank two (width x height). However, a fully-connected (standard) neural network only accepts vectors (Tensors of rank 1). Therefore, the following cell **vectorizes each image in the dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flatten image](images/flatten_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize each image\n",
    "def maybe_vectorize(dataset):\n",
    "  if len(dataset.shape) == 3:\n",
    "    return np.reshape(dataset, (dataset.shape[0], dataset.shape[1] * dataset.shape[2]))\n",
    "  else:\n",
    "    return dataset\n",
    "\n",
    "train_dataset = maybe_vectorize(train_dataset)\n",
    "valid_dataset = maybe_vectorize(valid_dataset)\n",
    "test_dataset = maybe_vectorize(test_dataset)\n",
    "\n",
    "print('Training dataset shape:', train_dataset.shape)\n",
    "print('Validation dataset shape:', valid_dataset.shape)\n",
    "print('Test dataset shape:', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the dataset labels (which record what letter is depicted on each image) are stores as integers where 0 represents letter A and 3 represents D. A neural network usually outputs a vector in which each element represents the probability that an input image belongs to a certain label. In order to train the neural network, you will need to compare the predicted probabilities with the correct label. To do this, it's convenient to turn each label into a vector with a one in the position that corresponds to its index and the rest set to zero. This is called **one-hot encoding**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one-hot encoding](images/one_hot_encoding.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode each label\n",
    "def maybe_turn_to_one_hot(labels, num_labels=4):\n",
    "  if len(labels.shape) == 1:\n",
    "    one_hot = np.zeros((labels.shape[0], num_labels))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "  else:\n",
    "    return labels\n",
    "\n",
    "train_labels = maybe_turn_to_one_hot(train_labels)\n",
    "valid_labels = maybe_turn_to_one_hot(valid_labels)\n",
    "test_labels = maybe_turn_to_one_hot(test_labels)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', valid_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected Neural Network ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following series of tasks, you will implement a classfication model from scratch. We recommend you to use the Jupyter notebook from the first tutorial as a reference and try to implement your model based on that. Alternatively, you check the reference notebook which has all the solution (except for Part 4 which contains a bonus-point task) but you won't learn much by copying them. The tasks aren't graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A ###\n",
    "\n",
    "* define a computation graph for a **fully-connected neural network**\n",
    "* the network should have **2 hidden layers** with **200 neurons in the first** and **100 neurons in the second** hidden layer\n",
    "* **input**: vectorized images in the shape (num_images, 784)\n",
    "* **output**: predictions in the shape (num_images, 4)\n",
    "\n",
    "See the reference notebook for a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B ###\n",
    "\n",
    "* train the neural network you defined above using **Batch Gradient Descent**\n",
    "* during each learning step, the network makes predictions for all 18000 training images and learns from its mistakes\n",
    "* evaluate the network on the notMNIST evaluation set\n",
    "\n",
    "\n",
    "* recommended learning rate: 0.1\n",
    "* recommended number of training steps: 60\n",
    "\n",
    "See the reference notebook for a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1C ###\n",
    "\n",
    "* train the neural network you defined above using **Stochastic Gradient Descent**\n",
    "* during each learning step, the network makes predictions for a single image and learns from its mistakes\n",
    "* evaluate the network on the notMNIST evaluation set\n",
    "\n",
    "\n",
    "* recommended learning rate: 0.01\n",
    "* recommended number of training steps: 5000\n",
    "\n",
    "See the reference notebook for a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1D ###\n",
    "\n",
    "* train the neural network you defined above using **Mini-batch Gradient Descent**\n",
    "* during each learning step, the network makes predictions for a small batch of images and learns from its mistakes\n",
    "* evaluate the network on the notMNIST evaluation set\n",
    "\n",
    "\n",
    "* recommended mini-batch size: 64\n",
    "* recommended learning rate: 0.05\n",
    "* recommended number of training steps: 2000\n",
    "\n",
    "See the reference notebook for a the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Saving models and visualizing learning in Tensorflow ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not very convenient to train your model each time you want to use it. On top of that, complex Computer Vision models take weeks to train.\n",
    "\n",
    "In this section, you will learn how to save and load a Tensorflow model. In addition, you will visualize how the loss and accuracy changes during the training of your neural network using Tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2A ###\n",
    "\n",
    "Copy the definition of your neural network and add the following lines to the end of the code snippet. You might need to change the names of the variables or delete the second line if you haven't defined an operation that measures the accuracy of your model.\n",
    "\n",
    "```\n",
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "summaries = tf.summary.merge_all()\n",
    "```\n",
    "\n",
    "The first two lines create summary operations for your model's loss and accuracy which will be recorded during each training step. The third line groups the two summaries together so that you have a single operation that is easy to work with.\n",
    "\n",
    "See the reference notebook for a the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2B ###\n",
    "\n",
    "Add these lines to the beginning of your training script.\n",
    "\n",
    "```\n",
    "saver = tf.train.Saver()\n",
    "summary_writer = tf.summary.FileWriter('data', graph=tf.get_default_graph())\n",
    "```\n",
    "\n",
    "You can save a summary using `summary_writer.add_summary(summary, global_step=step)` and the model using `saver.save(session, os.path.join('data/notMNIST-model-1'), global_step=step)`.\n",
    "\n",
    "See the reference notebook for a the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can **load** and evaluate your model using the following script. When saving a model, Tensorflow generates three different files (data, meta and index). To load a model, simply specify its path without the extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_your_model = None\n",
    "\n",
    "if path_to_your_model is None:\n",
    "  print(\"Please specify the path to your saved model.\")\n",
    "else:\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver.restore(session, path_to_your_model)\n",
    "\n",
    "    validation_accuracy = session.run(accuracy, feed_dict={\n",
    "      input_data: valid_dataset,\n",
    "      input_labels: valid_labels\n",
    "    })\n",
    "    print('Validation accuracy:', validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the training, call the following command.\n",
    "\n",
    "```\n",
    "tensorboard --logdir data\n",
    "```\n",
    "\n",
    "You must activate the virtual environment that contains your instalation of Tensorflow before you can call this command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed, some of the models you have trained earlier report training accuracy that is much higher than the validation or testing accuracy. This is due to the model **overfitting** on the training data. Overfitting can be mitigated using **regularization** techniques.\n",
    "\n",
    "**Dropout** is a popular technique that prevents overfitting by dropping some of the activations of a particular layer. Take a look at [dropout in Tensorflow](https://www.tensorflow.org/api_docs/python/tf/layers/dropout). You can add it into your computation graph as a layer similarly to the Dense layer.\n",
    "\n",
    "However, there is one more thing you need to do before you can use it. Dropout should drop activations only during training because we don't want to loose any information at random when we make predictions with the model. You will need to define a boolean Tensor that will tell the dropout layer if it's in the training or testing mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (bonus points) ###\n",
    "\n",
    "Add a dropout layer **before** the last Dense layer and make sure that activations are dropped only during training. Try changing the drop probability in order to obtain the best validation accuracy. Use the neural network you have defined above and train it with mini-batch gradient descent. Note that a model with a stronger regularization might need more training steps to converge properly.\n",
    "\n",
    "The solution will be added to the reference notebook after the end of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources   ##\n",
    "\n",
    "** Saving and restoring models in Tensorfow **\n",
    "* [tutorial](https://www.tensorflow.org/programmers_guide/saved_model)\n",
    "\n",
    "** Visualizing learning using Tensorboard **\n",
    "* [tutorial](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\n",
    "\n",
    "** Dropout **\n",
    "* [documentation](https://www.tensorflow.org/api_docs/python/tf/layers/dropout)\n",
    "* [paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
